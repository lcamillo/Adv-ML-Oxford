{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys, pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from black_box_alphavi import fit_q\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_error(X, y, index_train, index_test, i, dataset, alpha = 0.0):\n",
    "    \n",
    "    # load training and test data\n",
    "    X_train = X[ index_train, ]\n",
    "    y_train = y[ index_train ]\n",
    "    X_test = X[ index_test ]\n",
    "    y_test = y[ index_test ]\n",
    "\n",
    "    # standardize the data\n",
    "    scaler_X = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "    \n",
    "    mean_y_train = np.mean(y_train)\n",
    "    std_y_train = np.std(y_train)\n",
    "    y_train = (y_train - mean_y_train) / std_y_train\n",
    "    \n",
    "    y_train = np.array(y_train, ndmin = 2).reshape((-1, 1))\n",
    "    y_test = np.array(y_test, ndmin = 2).reshape((-1, 1))\n",
    "    \n",
    "    # We iterate the method \n",
    "    learning_rate = 0.001\n",
    "    v_prior = 1.0\n",
    "    batch_size = 32\n",
    "    epochs = 500\n",
    "    K = 100\n",
    "    hidden_layer_size = 50\n",
    "    start_time = time.time()\n",
    "    w, v_prior, get_error_and_ll = fit_q(X_train, y_train, hidden_layer_size, \n",
    "        batch_size, epochs, K, alpha, learning_rate, v_prior)\n",
    "    running_time = time.time() - start_time\n",
    "\n",
    "    # We obtain the test RMSE and the test ll\n",
    "    \n",
    "    error, ll = get_error_and_ll(w, v_prior, X_test, y_test, K, mean_y_train, std_y_train)\n",
    "        \n",
    "    return -ll, error, running_time\n",
    "\n",
    "# Write a function to get random splits for train index and test index\n",
    "def split_data(n):\n",
    "    \n",
    "    permutation = np.random.choice(range(n), n, replace = False)\n",
    "    end_train = round(n * 9.0 / 10)\n",
    "\n",
    "    index_train = permutation[ 0 : end_train ]\n",
    "    index_test = permutation[ end_train : n ]\n",
    "    index_train = list(map(int, index_train))\n",
    "    index_test = list(map(int, index_test))\n",
    "    \n",
    "    return index_train, index_test\n",
    "    \n",
    "\n",
    "# Write a function like this called 'main'\n",
    "def main(dataset, alpha, n_splits):\n",
    "    \n",
    "    print(\"    Dataset    |     Alpha     |   Number of Splits  \")\n",
    "    print(\"{0:15}|{1:15}|{2:15}\".format(dataset, alpha, n_splits))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # We load the data\n",
    "    datapath = 'data/' + dataset + '/'\n",
    "    data = np.loadtxt(datapath + 'data.txt')\n",
    "    index_features = np.loadtxt(datapath + 'index_features.txt')\n",
    "    index_target = np.loadtxt(datapath + 'index_target.txt')\n",
    "    \n",
    "    # We generate the training test splits\n",
    "\n",
    "    i_features = list(map(int, np.linspace(0,index_features.size-1, index_features.size)))\n",
    "    i_target = int(index_target)\n",
    "    X = data[ : , i_features ]\n",
    "    y = data[ : , i_target ]\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    savepath = datapath + 'results/'\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        \n",
    "        print(i/n_splits * 100, '% completed')\n",
    "        \n",
    "        np.random.seed(i)\n",
    "        index_train, index_test = split_data(n)\n",
    "        \n",
    "        neg_test_ll, test_error, running_time = get_test_error(X, y, index_train, index_test,\n",
    "                                                               i+1, dataset, alpha)\n",
    "        \n",
    "        with open(savepath + dataset + \"_test_ll_alpha{}.txt\".format(alpha), 'a') as f:\n",
    "            f.write(repr(neg_test_ll) + '\\n')\n",
    "        with open(savepath + dataset + \"_test_error_alpha{}.txt\".format(alpha), 'a') as f:\n",
    "            f.write(repr(test_error) + '\\n')\n",
    "        with open(savepath + dataset + \"_test_time_alpha{}.txt\".format(alpha), 'a') as f:\n",
    "            f.write(repr(running_time) + '\\n')\n",
    "            \n",
    "    print('100.0% completed')        \n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['boston', 'computer', 'concrete','energy', 'housing', \n",
    "               'power', 'slump', 'wine', 'yacht']\n",
    "alpha = [np.NINF, 0, 0.5, 1, 10**5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets:\n",
    "    for j in range(len(alpha)):\n",
    "        main(df, alpha[j], n_splits = 10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
